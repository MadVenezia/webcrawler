#!/usr/bin/env python3

import argparse
import socket
import ssl
import urllib.parse
import re
import gzip

# Default server and port
DEFAULT_SERVER = "www.3700.network"
DEFAULT_PORT = 443

class CustomWebCrawler:
    def __init__(self, args):
        """
        Initializes the CustomWebCrawler object with necessary attributes.

        Args:
            args: Command-line arguments passed to the script.
        """
        # Server, port, username, password
        self.server_address = args.server
        self.server_port = args.port
        self.user = args.user
        self.password = args.password

        # Tokens and flags
        self.csrf_token = None
        self.session_cookie = None
        self.middleware_token = None
        self.secret_flags = set()

        # Explored URLs and URLs to explore
        self.visited_urls = {'/', '/accounts/logout/'}
        self.urls_to_explore = set()

        # Socket
        self.ssl_socket = None

    def retrieve_tokens(self, data: str) -> tuple:
        """
        Extracts CSRF token, session cookie, and middleware token from response data.

        Args:
            data (str): Response data.

        Returns:
            tuple: CSRF token, session cookie, and middleware token.
        """
        data_headers = data.split('\n')

        for header in data_headers:
            if 'set-cookie: csrftoken=' in header:
                try:
                    self.csrf_token = header.split('csrftoken=')[1].split(';')[0].strip()
                except ValueError:
                    pass
            elif 'set-cookie: sessionid=' in header:
                try:
                    self.session_cookie = header.split('sessionid=')[1].split(';')[0].strip()
                except ValueError:
                    pass
            elif 'name="csrfmiddlewaretoken"' in header:
                try:
                    self.middleware_token = header.split('value="')[1].split('"')[0].strip()
                except ValueError:
                    pass

        return self.csrf_token, self.session_cookie, self.middleware_token

    def perform_login(self) -> str:
        """
        Generates the login request.

        Returns:
            str: Login request.
        """
        path = "/accounts/login/"
        params = {
                "user": self.user,
                "password": self.password,
                "csrfmiddlewaretoken": self.middleware_token,
                "next": "/fakebook/"
        }
        body = urllib.parse.urlencode(params)
        body += "\r\n\r\n"

        length = len(body)
        header = f"POST {path} HTTP/1.1\r\nHost: {self.server_address}\r\nAccept-Encoding: gzip\r\nContent-Type: application/x-www-form-urlencoded\r\nCookie: sessionid={self.session_cookie}; csrftoken={self.csrf_token}\r\nConnection: keep-alive\r\nContent-Length: {length}\r\n\r\n"

        login_request = header + body
        return login_request

    def read_response_data(self) -> str:
        """
        Reads data from the socket.

        Returns:
            str: Decoded data.
        """
        data = b""

        while True:
            chunk = self.ssl_socket.recv(256)
            data += chunk

            if not chunk:
                break

            elif data[-2:] == b'\x00\x00':
                header, body = data.split(b'\r\n\r\n', 1)
                break

        body = gzip.decompress(body)
        data = header + body
        return data.decode('ascii')

    def establish_ssl_socket(self) -> None:
        """
        Establishes a SSL socket connection.
        """
        context = ssl.SSLContext(ssl.PROTOCOL_TLS)
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.ssl_socket = context.wrap_socket(sock, server_hostname=self.server_address)
        self.ssl_socket.connect((self.server_address, self.server_port))

    def extract_hyperlinks(self, html: str) -> set:
        """
        Extracts hyperlinks from HTML content.

        Args:
            html (str): HTML content.

        Returns:
            set: Set of URLs.
        """
        pattern = r'<a\s.*?href="(.*?)".*?>(.*?)<\/a>'
        values = re.findall(pattern, html)

        urls = set()
        for value in values:
            url = value[0]
            if url and url not in self.visited_urls and url not in self.urls_to_explore:
                urls.add(url)

        return urls

    def find_secret_flags(self, url: str, html: str) -> None:
        """
        Finds secret flags in HTML content.

        Args:
            url (str): URL.
            html (str): HTML content.
        """
        pattern = r'<h3\s+class=\'secret_flag\'\s+style="color:red">FLAG:\s+([A-Za-z0-9]{64})<\/h3>'
        flags = re.findall(pattern, html)
        self.secret_flags.update(flags)

    def check_http_response_code(self, data: str) -> int:
        """
        Checks HTTP response codes.

        Args:
            data (str): Response data.

        Returns:
            int: HTTP response code.
        """
        data_headers = data.split('\n')

        for header in data_headers:
            if header.startswith('HTTP/1.1'):
                try:
                    response_code = int(header.split()[1])
                    return response_code
                except ValueError:
                    return -1

        return -1

    def traverse_urls(self) -> None:
        """
        Traverses URLs recursively.
        """
        if len(self.urls_to_explore) == 0:
            return

        new_urls = set()
        for url in self.urls_to_explore:
            get_request = f"GET {url} HTTP/1.1\r\nHost: {self.server_address}\r\nAccept-Encoding: gzip\r\nCookie: sessionid={self.session_cookie};\r\nConnection: keep-alive\r\n\r\n"

            while True:
                self.ssl_socket.send(get_request.encode('ascii'))
                html_data = self.read_response_data()

                response_code = self.check_http_response_code(html_data)

                if response_code in {200, 403, 404, 302}:
                    break
                elif response_code == 503:
                    continue
                else:
                    raise ValueError(f"ERROR: Unexpected response code {response_code}")

            if response_code == 404 or response_code == 403:
                self.visited_urls.add(url)
                new_urls.update(self.extract_hyperlinks(html_data))
                continue

            self.find_secret_flags(url, html_data)

            if len(self.secret_flags) == 5:
                return

            self.visited_urls.add(url)
            new_urls.update(self.extract_hyperlinks(html_data))

        self.urls_to_explore = new_urls
        self.traverse_urls()

    def run_custom_crawler(self):
        """
        Runs the custom web crawler.
        """
        self.establish_ssl_socket()
        
        home_path = "/login/?next=fakebook/"
        get_request = f"GET {login_path} HTTP/1.1\r\nHost: {self.server_address}\r\nAccept-Encoding: gzip\r\nConnection: keep-alive\r\n\r\n"
        
        self.ssl_socket.send(get_request.encode('ascii'))
        data = self.read_response_data()
        self.retrieve_tokens(data)

        post_data = self.perform_login_request()
        post_data = post_data.encode('ascii')
        self.ssl_socket.send(post_data)

        response_data = self.read_response_data()     
        self.retrieve_tokens(response_data)

        home_path = "/dashboard/"
        get_request = f"GET {home_path} HTTP/1.1\r\nHost: {self.server_address}\r\nAccept-Encoding: gzip\r\nCookie: sessionid={self.session_cookie};\r\nConnection: keep-alive\r\n\r\n"

        self.ssl_socket.send(get_request.encode('ascii'))
        data = self.read_response_data()

        home_page_urls = self.extract_hyperlinks(data)
        self.urls_to_explore.update(home_page_urls)
        self.traverse_urls()

        for flag in self.secret_flags:
            print(flag)



if __name__ == "__main__":
    # Argument parser for command-line interface
    parser = argparse.ArgumentParser(description='crawl ExampleSite')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('user', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()

    # Create CustomWebCrawler instance and run the crawler
    crawler = CustomWebCrawler(args)
    crawler.run_custom_crawler()
