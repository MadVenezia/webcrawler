#!/usr/bin/env python3

import argparse
import socket
import ssl
import urllib.parse
import re
import gzip

DEFAULT_SERVER = "www.3700.network"
DEFAULT_PORT = 443

class WebCrawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password

        self.token = None
        self.cookie = None
        self.sock = None

        self.csrf_token = None
        self.session_id = None
        self.middleware_token = None

        self.explored = {'/', '/accounts/logout/'}
        self.to_explore = set()
        self.flags = set()


    def locate_tokens(self, data: str) -> tuple:
        """Finds the CSRF tokens and session id to be used for logging in.
        
        Args:
            data: the data received as a response to a request
        Returns:
            a tuple of the tokens and session id found
        """
        data_headers = data.split('\n')

        # gets the csrf token and session id
        for header in data_headers:
            if 'set-cookie: csrftoken=' in header:
                try:
                    self.csrf_token = header.split('csrftoken=')[1].split(';')[0].strip()
                except ValueError:
                    return
            elif 'set-cookie: sessionid=' in header:
                try:
                    self.session_id = header.split('sessionid=')[1].split(';')[0].strip()
                except ValueError:
                    return
            elif 'name="csrfmiddlewaretoken"' in header:
                try:
                    self.middleware_token = header.split('value="')[1].split('"')[0].strip()
                except ValueError:
                    return
        
        return self.csrf_token, self.session_id, self.middleware_token


    def authenticate(self) -> str:
        """Makes a POST request to login to Fakebook.
        
        Returns:
            the POST request to be made
        """
        path = "/accounts/login/"
        params = {
                "username": self.username,
                "password": self.password,
                "csrfmiddlewaretoken": self.middleware_token,
                "next": "/fakebook/"
        }
        body = urllib.parse.urlencode(params)
        body += "\r\n\r\n"

        length = len(body)
        header = f"POST {path} HTTP/1.1\r\nHost: {self.server}\r\nAccept-Encoding: gzip\r\nContent-Type: application/x-www-form-urlencoded\r\nCookie: sessionid={self.session_id}; csrftoken={self.csrf_token}\r\nConnection: keep-alive\r\nContent-Length: {length}\r\n\r\n"

        # create the full header
        request = header + body
        return request


    def receive_data(self) -> str:
        """Reads data in from over the socket.

        Returns:
            the string of data read.
        """
        data = b""
        while True:
            chunk = self.sock.recv(256)
            data += chunk
           
            if not chunk: break
            # chatgpt recommended this for finding the end of a zipped file
            elif data[-2:] == b'\x00\x00':
                header, body = data.split(b'\r\n\r\n', 1)
                break

        body = gzip.decompress(body)
        data = header + body
        return data.decode('ascii')


    def receive_header(self) -> str:
        """Reads data in from over the socket. Only reads the header

        Returns:
            the string of data read.
        """
        data = ""
        while True:
            chunk = self.sock.recv(256).decode('ascii')
            data += chunk
            
            if not chunk: break
            elif "\r\n\r\n" in data: break

        return data


    def establish_connection(self) -> None:
        """Creates a TLS-wrapped socket"""
        context = ssl.SSLContext(ssl.PROTOCOL_TLS)
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.sock = context.wrap_socket(sock, server_hostname=self.server)
        self.sock.connect((self.server, self.port))


    def fetch_urls(self, html: str) -> set:
        """Traverses the given html and adds any unexplored urls to 
        a list to traverse.

        Args:
            html: the html to traverse for urls
        Returns:
            a set of unexplored urls
        """
        # chatgpt regex pattern for finding 'a' tags
        pattern = r'<a\s.*?href="(.*?)".*?>(.*?)<\/a>'
        values = re.findall(pattern, html)
        
        # only get urls
        urls = set()
        for value in values:
            url = value[0]
            if url and url not in self.explored and url not in self.to_explore:
                urls.add(url)
        
        return urls


    def find_flags(self, url: str, html: str) -> None:
        """Attempts to find secret flags within the given html.

        Args:
            url: the url of the page being traversed
            html: the html of the page to be searched
        """
        # chatpt regex pattern for flags
        pattern = r'<h3\s+class=\'secret_flag\'\s+style="color:red">FLAG:\s+([A-Za-z0-9]{64})<\/h3>'
        flags = re.findall(pattern, html)
        self.flags.update(flags)
        

    def inspect_response_codes(self, data: str) -> int:
        """Pulls the response code from the given page

        Args:
            data: the entire server response as a string
        Returns:
            the found response code or -1
        """
        data_headers = data.split('\n')

        for header in data_headers:
            if header.startswith('HTTP/1.1'):
                try:
                    response_code = int(header.split()[1])
                    return response_code
                except ValueError:
                    return -1

        return -1


    def explore(self) -> None:
        """Recursively crawls through each page of discovered but unexplored 
        urls, searching for secret flags and finding more urls to explore.
        """
        # nothing left to crawl
        if len(self.to_explore) == 0:
            return 
        
        # things left to crawl
        new_urls = set()
        for url in self.to_explore:
            # go to url
            get = f"GET {url} HTTP/1.1\r\nHost: {self.server}\r\nAccept-Encoding: gzip\r\nCookie: sessionid={self.session_id};\r\nConnection: keep-alive\r\n\r\n"

            # send until a 200 code is received
            while True:
                self.sock.send(get.encode('ascii'))
                
                # find any flags
                html = self.receive_data()

                # check codes
                code = self.inspect_response_codes(html)

                if code in {200, 403, 404, 302}:
                    break
                elif code == 503:
                    continue
                else:
                    raise ValueError(f"ERROR: Unexpected response code {code
}

            # drop the current url and move on
            if code == 404 or code == 403:
                self.explored.add(url)
                new_urls.update(self.fetch_urls(html))
                
                continue
            
            self.find_flags(url, html)

            # all flags found, end
            if len(self.flags) == 5:
                return

            # find new urls + update urls
            self.explored.add(url)
            new_urls.update(self.fetch_urls(html))

        # refresh list of urls left to explore
        self.to_explore = new_urls
        
        # recursively explore
        self.explore()
        

    def start_crawler(self):
        self.establish_connection()
        
        # make request to see login page
        path = "/accounts/login/?next=fakebook/"
        request = f"GET {path} HTTP/1.1\r\nHost: {self.server}\r\nAccept-Encoding: gzip\r\nConnection: keep-alive\r\n\r\n"
        
        # get csrf token and session id
        self.sock.send(request.encode('ascii'))
        data = self.receive_data()
        self.locate_tokens(data)

        # send POST login request
        post = self.authenticate()
        post = post.encode('ascii')
        self.sock.send(post)

        data = self.receive_header()
            
        self.locate_tokens(data)

        # get request to fakebook homepage
        home = "/fakebook/"
        get = f"GET {home} HTTP/1.1\r\nHost: {self.server}\r\nAccept-Encoding: gzip\r\nCookie: sessionid={self.session_id};\r\nConnection: keep-alive\r\n\r\n"

        self.sock.send(get.encode('ascii'))
        data = self.receive_data()

        # get initial batch of urls + start traversing
        home_page_urls = self.fetch_urls(data)
        self.to_explore.update(home_page_urls)
        self.explore()

        # print flags found
        for flag in self.flags:
            print(flag)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    crawler = WebCrawler(args)
    crawler.start_crawler()
